{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18dfd8e9",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a9938e",
   "metadata": {},
   "source": [
    "### by ReDay Zarra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141d52b",
   "metadata": {},
   "source": [
    "Data preprocessing is about **preparing data for analysis** by cleaning, transforming, and organizing it. This step is crucial for any machine learning or data analysis as it helps to ensure that the data is accurate, consistent, and comprehensive. Data preprocessing includes tasks such as handling missing values, removing outliers, normalizing data, and encoding categorical variables. The goal of data preprocessing is to **make the data as clean and usable** as possible, so that it can be used for further analysis and modeling.\n",
    "\n",
    "The three major steps of data preprocessing includes:\n",
    "\n",
    "1. Importing the necessary libraries\n",
    "\n",
    "2. Importing the dataset\n",
    "\n",
    "3. Addressing missing data\n",
    "\n",
    "4. Encoding categorical data\n",
    "\n",
    "5. Splitting the dataset\n",
    "\n",
    "6. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b178039",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838a16d",
   "metadata": {},
   "source": [
    "Importing the necessary libraries and modules we need to start preprocessing \n",
    "our data. **Pandas** is a library used for data frame manipulations. **NumPy** is a package used for numerical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034b753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c64fe05",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01c192",
   "metadata": {},
   "source": [
    "Importing the dataset requires us to use the **Pandas library which will import the dataset** in a new variable. Then we have to create the matrix of features and then the dependent variable vector. The features are the columns with which you will predict the final decision (dependent variable). So, the dependent variable is really what you want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103ea4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[: , :-1].values\n",
    "y = dataset.iloc[: , -1].values\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7313882",
   "metadata": {},
   "source": [
    "> Pandas reads the dataset and **creates a data frame** which is **stored in the variable dataset**. The **.iloc() function** means locating indices and **identifies the data we want to target**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6561a3",
   "metadata": {},
   "source": [
    "## Addressing missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ec29c",
   "metadata": {},
   "source": [
    "Our dataset has missing data and values for some of the features,\n",
    "which can cause errors in our machine learning models. To address that, there\n",
    "are some options:\n",
    "\n",
    "  1. Ignoring the missing data by simply **removing** missing data\n",
    "  \n",
    "  2. **Replacing** the missing data with the average of the column\n",
    "\n",
    "**Scikit-Learn** is a data science library with a lot of data\n",
    "preprocessing tools that aid us in replacing missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e383a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2887796",
   "metadata": {},
   "source": [
    "> Uses the SimpleImputer class to target the missing values with the **missing_values** parameter. The **strategy** parameter specifies that we want to replace it with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "409f31f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(X[: , 1:3])\n",
    "X[: , 1:3]  = imputer.transform(X[: , 1:3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef06692",
   "metadata": {},
   "source": [
    "> The **.fit()** method from the imputer **calculates** our SimpleImputer **for the data specified**. We then use the **.transform()** method **to change the original data** to the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ff2de",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca6c07",
   "metadata": {},
   "source": [
    "The dataset includes categorical data that does not have a sequence of orders. For example, in the first column we have the names of all the countries which is important but has no numerical significance. We have to **encode this data so the model can interpret it** correctly, and we do that with **one hot encoding**.\n",
    "\n",
    "One hot coding means **creating binary vectors for each unique value of categorical data**. For example, creating three different columns for 3 countries. The purchased column which contrains Yes and No's will be converted into binary.\n",
    "\n",
    "However, we must **avoid redundancy** in our categorical data, or the **dummy variable trap**. We can do be careful by removing one column but **sci-kit learn does this for us!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d7822c",
   "metadata": {},
   "source": [
    "### Encoding the independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "889a7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05d7a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0])], remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13c33f",
   "metadata": {},
   "source": [
    "> The **ColumnTransfomer** class **manipulates the data from an entire column** to our liking and it accepts these two parameters. **Transformers** - which is what we want to do, type of encoding we want to do, and on which columns. **Remainder** - the columns which we want to keep and don't want to apply transformations to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2359191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(transformer.fit_transform(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8f875",
   "metadata": {},
   "source": [
    "> The **.fit_transform() method identifies the correct indices and changes them** at the same time. However, the **output is not an array** which can be harmful for the model, so we use NumPy's **.array() method to force the ouptut as an array**. The output is a copy of X so we are reassigning it to the original X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078c9d8",
   "metadata": {},
   "source": [
    "### Encoding the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854c5ad",
   "metadata": {},
   "source": [
    "The dataset for the **dependent variable has to be encoded** because the dependent variable consists of **Yes's and No's which can be rewritten as 0's and 1's** for the model to better understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "825f6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y = le."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766a926",
   "metadata": {},
   "source": [
    "## Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26a53b",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
