{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68cc9ae7",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee6425",
   "metadata": {},
   "source": [
    "### by ReDay Zarra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915331da",
   "metadata": {},
   "source": [
    "Data preprocessing is about **preparing data for analysis** by cleaning, transforming, and organizing it. This step is crucial for any machine learning or data analysis as it helps to ensure that the data is accurate, consistent, and comprehensive. Data preprocessing includes tasks such as handling missing values, removing outliers, normalizing data, and encoding categorical variables. The goal of data preprocessing is to **make the data as clean and usable** as possible, so that it can be used for further analysis and modeling.\n",
    "\n",
    "The three major steps of data preprocessing includes:\n",
    "\n",
    "1. Importing the necessary libraries\n",
    "\n",
    "2. Importing the dataset\n",
    "\n",
    "3. Addressing missing data\n",
    "\n",
    "4. Encoding categorical data\n",
    "\n",
    "5. Splitting the dataset\n",
    "\n",
    "6. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb23844",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049eb05f",
   "metadata": {},
   "source": [
    "Importing the necessary libraries and modules we need to start preprocessing \n",
    "our data. **Pandas** is a library used for data frame manipulations. **NumPy** is a package used for numerical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8f811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7069474",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9be831",
   "metadata": {},
   "source": [
    "Importing the dataset requires us to use the **Pandas library which will import the dataset** in a new variable. Then we have to create the matrix of features and then the dependent variable vector. The features are the columns with which you will predict the final decision (dependent variable). So, the dependent variable is really what you want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9fc953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[: , :-1].values\n",
    "y = dataset.iloc[: , -1].values\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1755a14",
   "metadata": {},
   "source": [
    "> Pandas reads the dataset and **creates a data frame** which is **stored in the variable dataset**. The **.iloc() function** means locating indices and **identifies the data we want to target**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc1766",
   "metadata": {},
   "source": [
    "## Addressing missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189d28d",
   "metadata": {},
   "source": [
    "Our dataset has missing data and values for some of the features,\n",
    "which can cause errors in our machine learning models. To address that, there\n",
    "are some options:\n",
    "\n",
    "  1. Ignoring the missing data by simply **removing** missing data\n",
    "  \n",
    "  2. **Replacing** the missing data with the average of the column\n",
    "\n",
    "**Scikit-Learn** is a data science library with a lot of data\n",
    "preprocessing tools that aid us in replacing missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851cbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b280635",
   "metadata": {},
   "source": [
    "> Uses the SimpleImputer class to target the missing values with the **missing_values** parameter. The **strategy** parameter specifies that we want to replace it with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2669cc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(X[: , 1:3])\n",
    "X[: , 1:3]  = imputer.transform(X[: , 1:3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba921f11",
   "metadata": {},
   "source": [
    "> The **.fit()** method from the imputer **calculates** our SimpleImputer **for the data specified**. We then use the **.transform()** method **to change the original data** to the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b044b80",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4178ad",
   "metadata": {},
   "source": [
    "The dataset includes categorical data that does not have a sequence of orders. For example, in the first column we have the names of all the countries which is important but has no numerical significance. We have to **encode this data so the model can interpret it** correctly, and we do that with **one hot encoding**.\n",
    "\n",
    "One hot coding means **creating binary vectors for each unique value of categorical data**. For example, creating three different columns for 3 countries. The purchased column which contrains Yes and No's will be converted into binary.\n",
    "\n",
    "However, we must **avoid redundancy** in our categorical data, or the **dummy variable trap**. We can do be careful by removing one column but **sci-kit learn does this for us!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2588e0",
   "metadata": {},
   "source": [
    "### Encoding the independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b0114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69301ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0])], remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb0587",
   "metadata": {},
   "source": [
    "> The **ColumnTransfomer** class **manipulates the data from an entire column** to our liking and it accepts these two parameters. **Transformers** - which is what we want to do, type of encoding we want to do, and on which columns. **Remainder** - the columns which we want to keep and don't want to apply transformations to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03a10db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(transformer.fit_transform(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeecbd7",
   "metadata": {},
   "source": [
    "> The **.fit_transform() method identifies the correct indices and changes them** at the same time. However, the **output is not an array** which can be harmful for the model, so we use NumPy's **.array() method to force the ouptut as an array**. The output is a copy of X so we are reassigning it to the original X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316fc042",
   "metadata": {},
   "source": [
    "### Encoding the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4513d514",
   "metadata": {},
   "source": [
    "The dataset for the **dependent variable has to be encoded** because the dependent variable consists of **Yes's and No's which can be rewritten as 0's and 1's** for the model to better understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96c0d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22d2aaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c196e",
   "metadata": {},
   "source": [
    "> Initialize the variable **encoder as an object** of the LabelEncoder class. Then use the **.fit_transform() method to apply** the encoding on the y subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fc8cc",
   "metadata": {},
   "source": [
    "## Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33739fce",
   "metadata": {},
   "source": [
    "Splitting the dataset consists of **making two seperate sets - a training set and testing set** which will be used for training the model and evaluating it. Splitting the data is done **before feature scaling**, scaling all features to make them even, **because the test set is supposed to be a brand new set** to which you should be evaluating your machine learning model. You **must treat it like a brand new dataset so you cannot apply the same feature scaling** to the testing set to prevent information leakage.\n",
    "\n",
    "Use the **train_test_split function** from scikit-learn **to divide the data into four** sections - features and dependent variable sets for both the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea26c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ad9e60",
   "metadata": {},
   "source": [
    "We are now going to divide the X (features) and Y (dependent variable) randomly with a **random_state of 1 so we can replicate our results**. And making sure the **size of the testing set is 20%** meaning training set is 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d57a139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a09bdbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "932c7ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b9f2f",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1591c2",
   "metadata": {},
   "source": [
    "Feature scaling allows us to **scale our features evenly so some features do not dominate other features** (outliers). However, feature scaling is **often unncessary** for most machine learning models because the models use linear regression equations, for example: y = b0 + b1x1 + b2x2 + ... , and so the model adjusts for the features by choosing a smaller coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "706ceec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74ee686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[: , 3:] = scaler.fit_transform(X_train[: , 3:])\n",
    "X_test[: , 3:] = scaler.fit_transform(X_test[: , 3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f8316c",
   "metadata": {},
   "source": [
    "> Use the variable **scaler as an instance object** of the StandardScaler class, then use the **.fit_transform to apply** the scaler to the specified subset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
